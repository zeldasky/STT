"""
Macìš© í•œêµ­ì–´ ìŒì„±-í…ìŠ¤íŠ¸ ë³€í™˜ ì‹œìŠ¤í…œ (MPS ë¬¸ì œ í•´ê²° ë²„ì „)
"""

import torch
import torchaudio
import librosa
import soundfile as sf
from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC
from pyannote.audio import Pipeline
import warnings
import os
from pathlib import Path
from datetime import datetime
import json
import numpy as np

warnings.filterwarnings('ignore')


class KoreanSTTSystem:
    """í•œêµ­ì–´ ìŒì„± ì¸ì‹ ì‹œìŠ¤í…œ"""

    def __init__(self, model_name="kresnik/wav2vec2-large-xlsr-korean", huggingface_token=None, use_gpu=False):
        """
        Args:
            model_name: HuggingFace ëª¨ë¸ëª…
            huggingface_token: HuggingFace API í† í°
            use_gpu: GPU ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: False - CPU ì‚¬ìš©)
        """
        print("ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘...")

        # âš ï¸ MPS ë¬¸ì œ í•´ê²°: CPU ê°•ì œ ì‚¬ìš©
        if use_gpu:
            if torch.cuda.is_available():
                self.device = "cuda"
                print("ğŸš€ NVIDIA GPU ì‚¬ìš©")
            else:
                self.device = "cpu"
                print("ğŸ’» CPU ì‚¬ìš© (CUDA ë¯¸ì§€ì›)")
        else:
            self.device = "cpu"
            print("ğŸ’» CPU ì‚¬ìš© (ì•ˆì •ì„± ìš°ì„ )")

        # STT ëª¨ë¸ ë¡œë“œ
        try:
            self.processor = Wav2Vec2Processor.from_pretrained(
                model_name,
                token=huggingface_token
            )
            self.model = Wav2Vec2ForCTC.from_pretrained(
                model_name,
                token=huggingface_token
            ).to(self.device)
            print(f"âœ… STT ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {model_name}")
        except Exception as e:
            print(f"âŒ STT ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            print("ğŸ’¡ í•´ê²°ë°©ë²•: huggingface-cli login ì‹¤í–‰ í›„ í† í° ì…ë ¥")
            raise

        # í™”ì ë¶„ë¦¬ ëª¨ë¸ (ì„ íƒì‚¬í•­)
        self.diarization_pipeline = None
        if huggingface_token:
            try:
                self.diarization_pipeline = Pipeline.from_pretrained(
                    "pyannote/speaker-diarization-3.1",
                    use_auth_token=huggingface_token
                )
                print("âœ… í™”ì ë¶„ë¦¬ ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸ í™”ì ë¶„ë¦¬ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                print("ğŸ’¡ í™”ì ë¶„ë¦¬ ì—†ì´ STTë§Œ ì§„í–‰í•©ë‹ˆë‹¤.")

    def load_audio(self, audio_path, target_sr=16000):
        """ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        print(f"ğŸ“‚ ì˜¤ë””ì˜¤ ë¡œë”©: {audio_path}")

        # librosaë¡œ ì˜¤ë””ì˜¤ ë¡œë“œ
        audio, sr = librosa.load(audio_path, sr=target_sr, mono=True)

        print(f"âœ… ì˜¤ë””ì˜¤ ë¡œë“œ ì™„ë£Œ (ê¸¸ì´: {len(audio) / sr:.2f}ì´ˆ, SR: {sr}Hz)")
        return audio, sr

    def transcribe_audio(self, audio, sr=16000, chunk_length_s=30):
        """
        ì˜¤ë””ì˜¤ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ê¸´ ì˜¤ë””ì˜¤ëŠ” ì²­í¬ë¡œ ë¶„í• )

        Args:
            audio: ì˜¤ë””ì˜¤ ë°ì´í„° (numpy array)
            sr: ìƒ˜í”Œë§ ë ˆì´íŠ¸
            chunk_length_s: ì²­í¬ ê¸¸ì´ (ì´ˆ)

        Returns:
            str: ë³€í™˜ëœ í…ìŠ¤íŠ¸
        """
        audio_length = len(audio) / sr

        # ì§§ì€ ì˜¤ë””ì˜¤ëŠ” í•œ ë²ˆì— ì²˜ë¦¬
        if audio_length <= chunk_length_s:
            return self._transcribe_chunk(audio, sr)

        # ê¸´ ì˜¤ë””ì˜¤ëŠ” ì²­í¬ë¡œ ë¶„í•  ì²˜ë¦¬
        print(f"ğŸ“Š ê¸´ ì˜¤ë””ì˜¤ ê°ì§€ ({audio_length:.1f}ì´ˆ) - ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤...")

        chunk_samples = int(chunk_length_s * sr)
        transcriptions = []

        num_chunks = int(np.ceil(len(audio) / chunk_samples))

        for i in range(num_chunks):
            start_idx = i * chunk_samples
            end_idx = min((i + 1) * chunk_samples, len(audio))

            chunk = audio[start_idx:end_idx]

            print(f"  ì²˜ë¦¬ ì¤‘: {i + 1}/{num_chunks} ({start_idx / sr:.1f}s - {end_idx / sr:.1f}s)")

            text = self._transcribe_chunk(chunk, sr)
            transcriptions.append(text)

        return " ".join(transcriptions)

    def _transcribe_chunk(self, audio, sr=16000):
        """ë‹¨ì¼ ì²­í¬ ë³€í™˜"""
        # ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬
        input_values = self.processor(
            audio,
            sampling_rate=sr,
            return_tensors="pt",
            padding=True
        ).input_values.to(self.device)

        # ì¶”ë¡ 
        with torch.no_grad():
            logits = self.model(input_values).logits

        # ë””ì½”ë”©
        predicted_ids = torch.argmax(logits, dim=-1)
        transcription = self.processor.batch_decode(predicted_ids)[0]

        return transcription

    def diarize_speakers(self, audio_path):
        """í™”ì ë¶„ë¦¬ ìˆ˜í–‰"""
        if self.diarization_pipeline is None:
            print("âš ï¸ í™”ì ë¶„ë¦¬ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None

        print("ğŸ¤ í™”ì ë¶„ë¦¬ ì§„í–‰ ì¤‘...")

        try:
            diarization = self.diarization_pipeline(audio_path)

            segments = []
            for turn, _, speaker in diarization.itertracks(yield_label=True):
                segments.append({
                    'start': turn.start,
                    'end': turn.end,
                    'speaker': speaker
                })

            print(f"âœ… í™”ì ë¶„ë¦¬ ì™„ë£Œ: {len(set([s['speaker'] for s in segments]))}ëª… ê°ì§€")
            return segments

        except Exception as e:
            print(f"âŒ í™”ì ë¶„ë¦¬ ì‹¤íŒ¨: {e}")
            return None

    def transcribe_with_speakers(self, audio_path):
        """í™”ì ë¶„ë¦¬ + STT í†µí•© ìˆ˜í–‰"""
        # ì˜¤ë””ì˜¤ ë¡œë“œ
        audio, sr = self.load_audio(audio_path)

        # í™”ì ë¶„ë¦¬
        speaker_segments = self.diarize_speakers(audio_path)

        results = []

        if speaker_segments:
            # í™”ìë³„ë¡œ STT ìˆ˜í–‰
            print("ğŸ“ í™”ìë³„ í…ìŠ¤íŠ¸ ë³€í™˜ ì¤‘...")

            for i, segment in enumerate(speaker_segments):
                start_sample = int(segment['start'] * sr)
                end_sample = int(segment['end'] * sr)

                segment_audio = audio[start_sample:end_sample]

                if len(segment_audio) > sr * 0.5:  # 0.5ì´ˆ ì´ìƒë§Œ ì²˜ë¦¬
                    text = self.transcribe_audio(segment_audio, sr)

                    results.append({
                        'speaker': segment['speaker'],
                        'start': segment['start'],
                        'end': segment['end'],
                        'text': text.strip()
                    })

                    print(f"  [{segment['speaker']}] {segment['start']:.1f}s-{segment['end']:.1f}s: {text[:50]}...")

        else:
            # í™”ì ë¶„ë¦¬ ì—†ì´ ì „ì²´ STT
            print("ğŸ“ ì „ì²´ í…ìŠ¤íŠ¸ ë³€í™˜ ì¤‘...")
            text = self.transcribe_audio(audio, sr)

            results.append({
                'speaker': 'Speaker_0',
                'start': 0.0,
                'end': len(audio) / sr,
                'text': text.strip()
            })

        print(f"âœ… ë³€í™˜ ì™„ë£Œ: {len(results)}ê°œ ì„¸ê·¸ë¨¼íŠ¸")
        return results

    def save_results(self, results, output_dir="output"):
        """ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        Path(output_dir).mkdir(exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # 1. JSON ì €ì¥
        json_path = f"{output_dir}/transcript_{timestamp}.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ JSON ì €ì¥: {json_path}")

        # 2. í…ìŠ¤íŠ¸ ì €ì¥
        txt_path = f"{output_dir}/transcript_{timestamp}.txt"
        with open(txt_path, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("íšŒì˜ë¡ ìë™ ë³€í™˜ ê²°ê³¼\n")
            f.write(f"ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("=" * 80 + "\n\n")

            current_speaker = None
            for item in results:
                if item['speaker'] != current_speaker:
                    f.write(f"\n[{item['speaker']}]\n")
                    current_speaker = item['speaker']

                f.write(f"[{item['start']:.1f}s - {item['end']:.1f}s]\n")
                f.write(f"{item['text']}\n\n")

        print(f"ğŸ’¾ í…ìŠ¤íŠ¸ ì €ì¥: {txt_path}")

        # 3. ë§ˆí¬ë‹¤ìš´ ì €ì¥
        md_path = f"{output_dir}/transcript_{timestamp}.md"
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(f"# íšŒì˜ë¡\n\n")
            f.write(f"**ìƒì„± ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write(f"**ì°¸ì„ì**: {', '.join(set([r['speaker'] for r in results]))}\n\n")
            f.write("---\n\n")

            current_speaker = None
            for item in results:
                if item['speaker'] != current_speaker:
                    f.write(f"\n## {item['speaker']}\n\n")
                    current_speaker = item['speaker']

                f.write(f"**[{item['start']:.1f}s - {item['end']:.1f}s]**\n\n")
                f.write(f"{item['text']}\n\n")

        print(f"ğŸ’¾ ë§ˆí¬ë‹¤ìš´ ì €ì¥: {md_path}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""

    print("=" * 80)
    print("ğŸ™ï¸ Macìš© í•œêµ­ì–´ ìŒì„±-í…ìŠ¤íŠ¸ ë³€í™˜ ì‹œìŠ¤í…œ (MPS ë¬¸ì œ í•´ê²° ë²„ì „)")
    print("=" * 80)
    print()

    # HuggingFace í† í°
    hf_token = os.getenv("HUGGINGFACE_TOKEN")

    if not hf_token:
        print("âš ï¸ HuggingFace í† í°ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ í™”ì ë¶„ë¦¬ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´ í† í°ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        print("ğŸ’¡ í† í° ì—†ì´ëŠ” STTë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        print()
        use_token = input("í† í°ì„ ì…ë ¥í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower()
        if use_token == 'y':
            hf_token = input("HuggingFace í† í° ì…ë ¥: ").strip()

    # STT ì‹œìŠ¤í…œ ì´ˆê¸°í™” (use_gpu=Falseë¡œ CPU ê°•ì œ ì‚¬ìš©)
    try:
        stt_system = KoreanSTTSystem(
            model_name="kresnik/wav2vec2-large-xlsr-korean",
            huggingface_token=hf_token,
            use_gpu=False  # âš ï¸ CPU ê°•ì œ ì‚¬ìš©
        )
    except Exception as e:
        print(f"\nâŒ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        print("\nğŸ’¡ í•´ê²° ë°©ë²•:")
        print("1. í„°ë¯¸ë„ì—ì„œ 'huggingface-cli login' ì‹¤í–‰")
        print("2. https://huggingface.co/settings/tokens ì—ì„œ í† í° ë°œê¸‰")
        print("3. í† í° ì…ë ¥ í›„ ë‹¤ì‹œ ì‹œë„")
        return

    print()
    print("=" * 80)

    # ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ì…ë ¥
    audio_path = input("ğŸµ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ë“œë˜ê·¸ ì•¤ ë“œë¡­ ê°€ëŠ¥): ").strip()
    audio_path = audio_path.replace('\\ ', ' ').strip("'\"")

    if not os.path.exists(audio_path):
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {audio_path}")
        return

    print()
    print("=" * 80)
    print("ğŸš€ ë³€í™˜ ì‹œì‘...")
    print("=" * 80)
    print()

    try:
        # STT ìˆ˜í–‰
        results = stt_system.transcribe_with_speakers(audio_path)

        # ê²°ê³¼ ì €ì¥
        stt_system.save_results(results)

        print()
        print("=" * 80)
        print("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
        print("=" * 80)
        print()
        print("ğŸ“Š ê²°ê³¼ ìš”ì•½:")
        print(f"  - ì´ ì„¸ê·¸ë¨¼íŠ¸: {len(results)}ê°œ")
        print(f"  - í™”ì ìˆ˜: {len(set([r['speaker'] for r in results]))}ëª…")
        print(f"  - ì´ í…ìŠ¤íŠ¸ ê¸¸ì´: {sum([len(r['text']) for r in results])}ì")
        print()
        print("ğŸ“ ì €ì¥ëœ íŒŒì¼: output/ ë””ë ‰í† ë¦¬ í™•ì¸")

    except Exception as e:
        print(f"\nâŒ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
